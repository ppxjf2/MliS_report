\section{Evaluation}
Each assessment was done with the following target coordinates.

\begin{table}[]
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
    \hline
    \textbf{Target} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\ \hline
    \textbf{x}      & 0.35       & -0.35      & 0.5        & -0.35      & 0.35       & -0.15      & -0.35      & 0.35       & -0.5       & 0.35        \\ \hline
    \textbf{y}      & 0.3        & 0.4        & -0.4       & 0          & 0.4        & -0.1       & -0.3       & -0.4       & 0.4        & 0           \\ \hline
    \end{tabular}
\end{table}

Each simulation was also run for 3000 simulation steps.

In this simulation, the heuristic algorithm hit 5 targets and achieved a value for the sum of rewards as 4894.42, against our chosen reward function.
The following graph shows an example of the Soft state Monti-Carlo state machine algorithmâ€™s results, under the same conditions. This is not a deterministic algorithm and therefore there is some variance  between iterations.

We modelled the results for the sum of the reward function for any given epoch through minimising the error with a log function.

