\section{Evaluation}
Each assessment was done with the following target coordinates.
(0.35, 0.3), (-0.35, 0.4), (0.5, -0.4), (-0.35, 0), (0.35, 0.4), (-0.15, -0.1), (-0.35, -0.3), (0.35, -0.4), (-0.5, 0.4), (0.35, 0).

Each simulation was also run for 3000 simulation steps.

In this simulation, the heuristic algorithm hit 5 targets and achieved a value for the sum of rewards as 4894.42, against our chosen reward function.
The following graph shows an example of the Soft state Monti-Carlo state machine algorithmâ€™s results, under the same conditions. This is not a deterministic algorithm and therefore there is some variance  between iterations.

We modelled the results for the sum of the reward function for any given epoch through minimising the error with a log function.

