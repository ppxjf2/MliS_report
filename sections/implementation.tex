\section{Implementation}
For the implementation for this project we used a Soft policy Monti Carlo algorithm implementing a state-action value function ensuring that all actions can be taken with a non-zero probability. 

\subsection{States}
The states for this model uses the pitch of the drone, distance from the target both x and y, and velocity. All these values are scaled to be integers within certain bounds and rounded down to fit within a grid which is used for action selection.  

\subsection{Actions}
There are 6 actions used within this program to define what movement the drone does.

The first action is used to counteract the drones current pitch if it is facing the wrong direction from the target. 

The second action adjusts the thrusts based on where the target is in the space and the angle of change that the drone will need to adjust to be directed at the target multiplied by 2. If the target is to the left of the drone it will have a lower left thrust than right thrust and vice versa. 

The third action gets the thrust based on the velocity of the drone's pitch to adjust which thrust it needs to be more to counteract the current rotation when it’s not rotating towards the target. This action is unstable compared to another action which does the same thing just at a slower rate.

The fourth action changes the thrust depending on the current pitch velocity of the drone. If the pitch velocity is in the direction of the target then the thrusts will more equal to slow down how much it spins, however if the pitch velocity is going in the opposite direction of the target then it strongly sets the thrusts to alter the pitch velocity to be towards the target, spinning it into the correct direction.

The fifth action adjusts the thrusts based on where the target is in the space and the angle of change that the drone will need to adjust to be directed at the target. If the target is to the left of the drone it will have a lower left thrust than right thrust and vice versa. 

The final action is used to adjust the drones y value. If the drones y value is lower than the target's y value then the drone will ascend and if the drones y value is above the target's y value then the drone will descend. 

The actions seem to be in a random order, this is due to a bug in which having a certain section of code in the final action, it would increment the pitch of the drone object exponentially even though there was no part of the code that would change that variable.

\subsection{Rewards}
We use the following reward function:

% R = -3a + 0.1b - c + 2(d^3) +2000(isrewardhit)
% Where 
% a = dist
% b = |drone.velocity_x| 
% c =  25( ⌊ |drone.pitch| ⌋ )
% d = targets hit

$R=\frac{(-3a+0.1b-c+2d^3)}{100}$

Where:

$a=\left \| Target Distance \right \|$

$b=abs(Drone Velocity)$

$c= \left \lfloor abs(DronePitch)  \right \rfloor $

$d=1+TargetsHit$

