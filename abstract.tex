This paper outlines the process and intricacies that were explored with creating a drone that is controlled autonomously through reinforcement methods. The method implemented is a state-action value algorithm with a soft policy to ensure all states have a non-zero probability of being taken. This is kept to the bounds of a defined space and set of targets and a specified amount of timesteps. The result followed a logarithmic line of best fit for the training, allowing the drone to work better than the heuristic provided on average, and if trained for 12395 epochs then any result worse than the heuristic would be anomalous. 